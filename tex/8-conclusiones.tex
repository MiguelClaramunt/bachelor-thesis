A continuación vamos a analizar cómo se han cumplido cada uno de los objetivos propuestos en la sección \hyperref[sec:objetivos]{1.2}:

\textbf{Definir correctamente el término \emph{fake news} y delimitar el área de estudio a tratar.} En el capítulo \hyperref[sec:defs]{2} hemos hecho una revisión bibliográfica, tomando diferentes definiciones del término y analizando los diferentes matices que aportan cada uno, decantándonos finalmente por el término de \textit{disinformation}, trabajando solamente noticias. Sabemos que nos dejamos muchas otras formas de \textit{disinformation}, pero es inevitable hacerlo debido a la naturaleza del Trabajo de Fin de Grado, es por ello que instamos a seguir investigando en el tema.

\textbf{Desarrollar una solución que permita clasificar noticias, sirviendo como triaje en el proceso de \textit{fact-checking}.} Se han propuesto diferentes modelos de lenguaje y \textit{datasets} y, viendo los resultados obtenidos en la sección \hyperref[sec:results]{6.1}, consideramos que hemos desarrollado un clasificador de noticias aunque los resultados no han sido los esperados. Los modelos LARGE, {DistilBERT}\textsubscript{B, C, ML}; {BERT}\textsubscript{B, C} y {DeBERTa}\textsubscript{B} son los que mejor funcionan en este problema, aunque creemos que los modelos implementados destacan más por sus métricas de \textit{specificity}, es decir, por su capacidad de distinguir qué características destacan en las noticias verdaderas. Aún así, por las métricas obtenidas en general, hace falta más trabajo para desarrollar una implementación que sea competente en el estado del arte actual.

\textbf{Hacer un estudio comparativo de los diferentes factores de los modelos y su aprendizaje, analizando la forma en la que afectan a su rendimiento.} En el capítulo \hyperref[chapter:6]{6} hemos comparado los diferentes resultados obtenidos y buscado una justificación plausible a este comportamiento. A grandes rasgos, el tamaño del modelo es el factor que parece contribuir más a la mejora de rendimiento de este. Lo segundo que parece afectar más es el hecho de si el modelo es multilingüe o distingue entre palabras capitalizadas. Por último, el tamaño del \textit{dataset} parece ser lo que menos contribuye, aunque hay que considerar la capacidad de generalización del modelo.

\textbf{Aplicar técnicas de \textit{Explainable AI} para entender el funcionamiento interno de los modelos en el proceso de clasificación.} En la sección \hyperref[sec:shap]{6.2} aplicamos estas técnicas mediante la librería \texttt{shap} y, mediante los valores Shapley, modelizamos la atención o la importancia que le da el modelo a ciertos \textit{tokens} en la clasificación. Concluimos que el comportamiento que tienen estos modelos no es el esperado, funcionando mejor en los \textit{datasets} {P-S}\textsubscript{One} y {P-S}\textsubscript{All} con menor información y número total de muestras que News. 
\newpage
\textbf{Analizar como afectan los sesgos en esta familia de modelos a los resultados obtenidos.} Como hemos observado en el apartado \hyperref[sec:biases]{7.2}, gracias a una revisión bibliográfica hemos conseguido desvelar sesgos en los LLMs. Estos no solo se limitan a una representación como \textit{word embedding} que puede dar interpretación a sesgos o prejuicios, sino que prácticamente estos sesgos son inherentes al proceso de aprendizaje. Es por ello que hay que tener en cuenta todos estos factores para realizar un análisis correcto de los resultados y de esta forma evitar perpetuando estos sesgos de forma directa e indirecta.